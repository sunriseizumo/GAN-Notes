{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code is from https://github.com/hardikbansal/CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imort module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "# from scipy.misc import imsave # deprecated. use imageio.imwrite() instead\n",
    "import imageio \n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import time\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import .layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\", alt_relu_impl=False):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        if alt_relu_impl:\n",
    "            f1 = 0.5 * (1 + leak)\n",
    "            f2 = 0.5 * (1 - leak)\n",
    "            # lrelu = 1/2 * (1 + leak) * x + 1/2 * (1 - leak) * |x|\n",
    "            return f1 * x + f2 * abs(x)\n",
    "        else:\n",
    "            return tf.maximum(x, leak*x)\n",
    "\n",
    "def instance_norm(x):\n",
    "\n",
    "    with tf.variable_scope(\"instance_norm\"):\n",
    "        epsilon = 1e-5\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        scale = tf.get_variable('scale',[x.get_shape()[-1]], \n",
    "            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
    "        offset = tf.get_variable('offset',[x.get_shape()[-1]],initializer=tf.constant_initializer(0.0))\n",
    "        out = scale*tf.div(x-mean, tf.sqrt(var+epsilon)) + offset\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def general_conv2d(inputconv, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"conv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        conv = tf.contrib.layers.conv2d(inputconv, o_d, f_w, s_w, padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
    "        if do_norm:\n",
    "            conv = instance_norm(conv)\n",
    "            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
    "            \n",
    "        if do_relu:\n",
    "            if(relufactor == 0):\n",
    "                conv = tf.nn.relu(conv,\"relu\")\n",
    "            else:\n",
    "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
    "\n",
    "        return conv\n",
    "\n",
    "\n",
    "\n",
    "def general_deconv2d(inputconv, outshape, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"deconv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        conv = tf.contrib.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        if do_norm:\n",
    "            conv = instance_norm(conv)\n",
    "            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
    "            \n",
    "        if do_relu:\n",
    "            if(relufactor == 0):\n",
    "                conv = tf.nn.relu(conv,\"relu\")\n",
    "            else:\n",
    "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
    "\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import .model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import numpy as np\n",
    "# from scipy.misc import imsave\n",
    "# import os\n",
    "# import shutil\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import random\n",
    "\n",
    "\n",
    "# from layers import *\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_layer = 3\n",
    "img_size = img_height * img_width\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "pool_size = 50\n",
    "ngf = 32\n",
    "ndf = 64\n",
    "\n",
    "\n",
    "\n",
    "def build_resnet_block(inputres, dim, name=\"resnet\"):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c1\")\n",
    "        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c2\",do_relu=False)\n",
    "        \n",
    "        return tf.nn.relu(out_res + inputres)\n",
    "\n",
    "\n",
    "def build_generator_resnet_6blocks(inputgen, name=\"generator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 7\n",
    "        ks = 3\n",
    "        \n",
    "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
    "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
    "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
    "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
    "\n",
    "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
    "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
    "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
    "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
    "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
    "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
    "\n",
    "        o_c4 = general_deconv2d(o_r6, [batch_size,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
    "        o_c5 = general_deconv2d(o_c4, [batch_size,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
    "        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
    "        o_c6 = general_conv2d(o_c5_pad, img_layer, f, f, 1, 1, 0.02,\"VALID\",\"c6\",do_relu=False)\n",
    "\n",
    "        # Adding the tanh layer\n",
    "\n",
    "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
    "\n",
    "\n",
    "        return out_gen\n",
    "\n",
    "def build_generator_resnet_9blocks(inputgen, name=\"generator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 7\n",
    "        ks = 3\n",
    "        \n",
    "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
    "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
    "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
    "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
    "\n",
    "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
    "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
    "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
    "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
    "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
    "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
    "        o_r7 = build_resnet_block(o_r6, ngf*4, \"r7\")\n",
    "        o_r8 = build_resnet_block(o_r7, ngf*4, \"r8\")\n",
    "        o_r9 = build_resnet_block(o_r8, ngf*4, \"r9\")\n",
    "\n",
    "        o_c4 = general_deconv2d(o_r9, [batch_size,128,128,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
    "        o_c5 = general_deconv2d(o_c4, [batch_size,256,256,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
    "        o_c6 = general_conv2d(o_c5, img_layer, f, f, 1, 1, 0.02,\"SAME\",\"c6\",do_relu=False)\n",
    "\n",
    "        # Adding the tanh layer\n",
    "\n",
    "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
    "\n",
    "\n",
    "        return out_gen\n",
    "\n",
    "\n",
    "def build_gen_discriminator(inputdisc, name=\"discriminator\"):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        f = 4\n",
    "\n",
    "        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=False, relufactor=0.2)\n",
    "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
    "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
    "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, \"SAME\", \"c4\",relufactor=0.2)\n",
    "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
    "\n",
    "        return o_c5\n",
    "\n",
    "\n",
    "def patch_discriminator(inputdisc, name=\"discriminator\"):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        f= 4\n",
    "\n",
    "        patch_input = tf.random_crop(inputdisc,[1,70,70,3])\n",
    "        o_c1 = general_conv2d(patch_input, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=\"False\", relufactor=0.2)\n",
    "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
    "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
    "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 2, 2, 0.02, \"SAME\", \"c4\", relufactor=0.2)\n",
    "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
    "\n",
    "        return o_c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import .main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import numpy as np\n",
    "# from scipy.misc import imsave\n",
    "# import os\n",
    "# import shutil\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import random\n",
    "# import sys\n",
    "\n",
    "# from layers import *\n",
    "# from model import *\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_layer = 3\n",
    "img_size = img_height * img_width\n",
    "\n",
    "to_train = True\n",
    "to_test = False\n",
    "to_restore = False\n",
    "output_path = \"./output\"\n",
    "check_dir = \"./output/checkpoints/\"\n",
    "\n",
    "\n",
    "temp_check = 0\n",
    "\n",
    "\n",
    "\n",
    "max_epoch = 1\n",
    "max_images = 100\n",
    "\n",
    "h1_size = 150\n",
    "h2_size = 300\n",
    "z_size = 100\n",
    "batch_size = 1\n",
    "pool_size = 50\n",
    "sample_size = 10\n",
    "save_training_images = True\n",
    "ngf = 32\n",
    "ndf = 64\n",
    "\n",
    "class CycleGAN():\n",
    "\n",
    "    def input_setup(self):\n",
    "\n",
    "        ''' \n",
    "        This function basically setup variables for taking image input.\n",
    "        filenames_A/filenames_B -> takes the list of all training images\n",
    "        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\n",
    "        '''\n",
    "\n",
    "        filenames_A = tf.train.match_filenames_once(\"./horse2zebra/trainA/*.jpg\")    \n",
    "        self.queue_length_A = tf.size(filenames_A)\n",
    "        filenames_B = tf.train.match_filenames_once(\"./horse2zebra/trainB/*.jpg\")    \n",
    "        self.queue_length_B = tf.size(filenames_B)\n",
    "        \n",
    "        filename_queue_A = tf.train.string_input_producer(filenames_A)\n",
    "        filename_queue_B = tf.train.string_input_producer(filenames_B)\n",
    "\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        _, image_file_A = image_reader.read(filename_queue_A)\n",
    "        _, image_file_B = image_reader.read(filename_queue_B)\n",
    "\n",
    "        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A),[256,256]),127.5),1)\n",
    "        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B),[256,256]),127.5),1)\n",
    "\n",
    "    \n",
    "\n",
    "    def input_read(self, sess):\n",
    "\n",
    "\n",
    "        '''\n",
    "        It reads the input into from the image folder.\n",
    "        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\n",
    "        self.A_input/self.B_input -> Stores all the training images in python list\n",
    "        '''\n",
    "\n",
    "        # Loading images into the tensors\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        num_files_A = sess.run(self.queue_length_A)\n",
    "        num_files_B = sess.run(self.queue_length_B)\n",
    "\n",
    "        self.fake_images_A = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
    "        self.fake_images_B = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
    "\n",
    "\n",
    "        self.A_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
    "        self.B_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
    "\n",
    "        for i in range(max_images): \n",
    "            image_tensor = sess.run(self.image_A)\n",
    "            if(image_tensor.size == img_size*batch_size*img_layer):\n",
    "                self.A_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
    "\n",
    "        for i in range(max_images):\n",
    "            image_tensor = sess.run(self.image_B)\n",
    "            if(image_tensor.size == img_size*batch_size*img_layer):\n",
    "                self.B_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
    "\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def model_setup(self):\n",
    "\n",
    "        ''' This function sets up the model to train\n",
    "        self.input_A/self.input_B -> Set of training images.\n",
    "        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\n",
    "        self.lr -> Learning rate variable\n",
    "        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. This is use to calcualte cyclic loss\n",
    "        '''\n",
    "\n",
    "        self.input_A = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_A\")\n",
    "        self.input_B = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_B\")\n",
    "        \n",
    "        self.fake_pool_A = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_A\")\n",
    "        self.fake_pool_B = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_B\")\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        self.num_fake_inputs = 0\n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        with tf.variable_scope(\"Model\") as scope:\n",
    "            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=\"g_A\")\n",
    "            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=\"g_B\")\n",
    "            self.rec_A = build_gen_discriminator(self.input_A, \"d_A\")\n",
    "            self.rec_B = build_gen_discriminator(self.input_B, \"d_B\")\n",
    "\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            self.fake_rec_A = build_gen_discriminator(self.fake_A, \"d_A\")\n",
    "            self.fake_rec_B = build_gen_discriminator(self.fake_B, \"d_B\")\n",
    "            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, \"g_B\")\n",
    "            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, \"g_A\")\n",
    "\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, \"d_A\")\n",
    "            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, \"d_B\")\n",
    "\n",
    "    def loss_calc(self):\n",
    "\n",
    "        ''' In this function we are defining the variables for loss calcultions and traning model\n",
    "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
    "        g_loss_A/g_loss_B -> loss for generator A/B\n",
    "        *_trainer -> Variaous trainer for above loss functions\n",
    "        *_summ -> Summary variables for above loss functions'''\n",
    "\n",
    "        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\n",
    "        \n",
    "        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))\n",
    "        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))\n",
    "        \n",
    "        g_loss_A = cyc_loss*10 + disc_loss_B\n",
    "        g_loss_B = cyc_loss*10 + disc_loss_A\n",
    "\n",
    "        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1))) / 2.0\n",
    "        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1))) / 2.0\n",
    "\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n",
    "\n",
    "        self.model_vars = tf.trainable_variables()\n",
    "\n",
    "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
    "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
    "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
    "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
    "        \n",
    "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
    "\n",
    "        for var in self.model_vars: print(var.name)\n",
    "\n",
    "        #Summary variables for tensorboard\n",
    "\n",
    "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
    "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
    "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
    "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
    "\n",
    "    def save_training_images(self, sess, epoch):\n",
    "\n",
    "        if not os.path.exists(\"./output/images/train/\"):\n",
    "            os.makedirs(\"./output/images/train/\")\n",
    "\n",
    "        for i in range(0,10):\n",
    "            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
    "            imageio.imwrite(\"./output/images/train/fakeB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
    "            imageio.imwrite(\"./output/images/train/fakeA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
    "            imageio.imwrite(\"./output/images/train/cycA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n",
    "            imageio.imwrite(\"./output/images/train/cycB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\n",
    "            imageio.imwrite(\"./output/images/train/inputA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "            imageio.imwrite(\"./output/images/train/inputB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "\n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        ''' This function saves the generated image to corresponding pool of images.\n",
    "        In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
    "        already stored image and replace it with new one.'''\n",
    "\n",
    "        if(num_fakes < pool_size):\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else :\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0,pool_size-1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else :\n",
    "                return fake\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        ''' Training Function '''\n",
    "\n",
    "\n",
    "        # Load Dataset from the dataset folder\n",
    "        self.input_setup()  \n",
    "\n",
    "        #Build the network\n",
    "        self.model_setup()\n",
    "\n",
    "        #Loss function calculations\n",
    "        self.loss_calc()\n",
    "      \n",
    "        # Initializing the global variables\n",
    "        init = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        saver = tf.train.Saver()     \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            #Read input to nd array\n",
    "            self.input_read(sess)\n",
    "\n",
    "            #Restore the model to run the model from last checkpoint\n",
    "            if to_restore:\n",
    "                chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
    "                saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            writer = tf.summary.FileWriter(\"./output/2\")\n",
    "\n",
    "            if not os.path.exists(check_dir):\n",
    "                os.makedirs(check_dir)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(sess.run(self.global_step),100):                \n",
    "                print (\"In the epoch \", epoch)\n",
    "                saver.save(sess,os.path.join(check_dir,\"cyclegan\"),global_step=epoch)\n",
    "\n",
    "                # Dealing with the learning rate as per the epoch number\n",
    "                if(epoch < 100) :\n",
    "                    curr_lr = 0.0002\n",
    "                else:\n",
    "                    curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n",
    "\n",
    "                if(save_training_images):\n",
    "                    self.save_training_images(sess, epoch)\n",
    "\n",
    "                # sys.exit()\n",
    "\n",
    "                for ptr in range(0,max_images):\n",
    "                    print(\"In the iteration \",ptr)\n",
    "                    print(\"Starting\",time.time()*1000.0)\n",
    "\n",
    "                    # Optimizing the G_A network\n",
    "\n",
    "                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
    "                    \n",
    "                    writer.add_summary(summary_str, epoch*max_images + ptr)                    \n",
    "                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
    "                    \n",
    "                    # Optimizing the D_B network\n",
    "                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_B:fake_B_temp1})\n",
    "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
    "                    \n",
    "                    \n",
    "                    # Optimizing the G_B network\n",
    "                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
    "\n",
    "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
    "                    \n",
    "                    \n",
    "                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
    "\n",
    "                    # Optimizing the D_A network\n",
    "                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\n",
    "\n",
    "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
    "                    \n",
    "                    self.num_fake_inputs+=1\n",
    "            \n",
    "                        \n",
    "\n",
    "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
    "\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "    def test(self):\n",
    "\n",
    "\n",
    "        ''' Testing Function'''\n",
    "\n",
    "        print(\"Testing the results\")\n",
    "\n",
    "        self.input_setup()\n",
    "\n",
    "        self.model_setup()\n",
    "        saver = tf.train.Saver()\n",
    "        init = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            self.input_read(sess)\n",
    "\n",
    "            chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            if not os.path.exists(\"./output/images/test/\"):\n",
    "                os.makedirs(\"./output/images/test/\")            \n",
    "\n",
    "            for i in range(0,100):\n",
    "                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
    "                imageio.imwrite(\"./output/images/test/fakeB_\"+str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
    "                imageio.imwrite(\"./output/images/test/fakeA_\"+str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
    "                imageio.imwrite(\"./output/images/test/inputA_\"+str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "                imageio.imwrite(\"./output/images/test/inputB_\"+str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    model = CycleGAN()\n",
    "    if to_train:\n",
    "        model.train()\n",
    "    elif to_test:\n",
    "        model.test()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
